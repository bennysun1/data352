# -*- coding: utf-8 -*-
"""Water_Safety_dense_neural_net

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1exOqBFvK5M-KCqOL3Vyurtucfsp0EZS7
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.layers import Dense, BatchNormalization, Dropout
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler
import matplotlib.pyplot as plt
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import cross_val_score

# To remove the scientific notation from numpy arrays
#np.set_printoptions(suppress=True)

# Load the data
water = pd.read_csv("https://raw.githubusercontent.com/bennysun1/data352/data/waterQuality1.csv", na_values = "#NUM!")
water = water.dropna()

# Separate Target Variable and Predictor Variables
target_variable = 'is_safe'
predictors = ['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',
              'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',
              'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',
              'silver', 'uranium']

X = water[predictors].values
y = water[target_variable].values

# Scale the data
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""# Optimization 1"""

# make a function to adjust learning rate based off the
# epoch number
def lr_schedule(epoch):
    if epoch < 15:
        return 0.01
    elif epoch < 35:
        return 0.001
    else:
        return 0.0001

"""Optimization 1: 5 fold cross validation"""

from sklearn.model_selection import cross_validate
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import KFold
from sklearn.model_selection import StratifiedKFold

# Define the number of folds for cross-validation
k = 5

# Define the KFold object
kf = KFold(n_splits=k, shuffle=True, random_state=42)

folds = StratifiedKFold(n_splits=k)

train_accuracy_scores = []
val_accuracy_scores = [] 
train_loss_scores = []
val_loss_scores = []
train_accuracy_history = []
val_accuracy_history = []
train_loss_history = []
val_loss_history = [] 

# Loop over each fold
for train_index, val_index in kf.split(X):

    tf.random.set_seed(42)

    # Split the data into training and validation sets for this fold
    # 80-20 train-val
    X_train, y_train = X[train_index], y[train_index]
    X_val, y_val = X[val_index], y[val_index]

    # Define the model architecture
    model1_cv = tf.keras.Sequential([
        Dense(64, activation='relu', input_shape=(X.shape[1],)),
        BatchNormalization(),
        Dropout(0.2),
        Dense(32, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(8, activation='relu'),
        BatchNormalization(),
        Dense(1, activation='sigmoid')
    ])

    # Compile the model
    optimizer = tf.keras.optimizers.Adam(lr=0.01)

    # Compile the model with adam optimizer
    model1_cv.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    # Define the callbacks
    early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)
    lr_scheduler = LearningRateScheduler(lr_schedule)

    # Train the model
    history1 = model1_cv.fit(X_train, y_train, epochs=100, batch_size=32,
                             validation_data=(X_val, y_val),
                             callbacks=[early_stopping, lr_scheduler])

    train_accuracy_history.append(history1.history['accuracy'])
    val_accuracy_history.append(history1.history['val_accuracy']) 
    train_loss_history.append(history1.history['loss'])
    val_loss_history.append(history1.history['val_loss'])

    # Evaluate the model on the validation set for this fold
    score = model1_cv.evaluate(X_val, y_val, verbose=0)
    val_loss_scores.append(score[0])
    val_accuracy_scores.append(score[1])

# Calculate the mean and standard deviation of the scores across all folds
mean_val_acc = np.mean(val_accuracy_scores)
std_val_acc = np.std(val_accuracy_scores)
mean_val_loss = np.mean(val_loss_scores)
std_val_loss = np.std(val_loss_scores)


print("Mean Validation Accuracy over", k, "folds:", mean_val_acc)
print("Std of Validation Accuracies over", k, "folds:", std_val_acc)
print("Mean Validation Loss over", k, "folds:", mean_val_loss)
print("Std of Validation Losses over", k, "folds:", std_val_loss)

tf.keras.utils.plot_model(
model1_cv,
to_file="model.png",
show_shapes=True,
show_dtype=False,
show_layer_names=True,
rankdir="TB",
expand_nested=True,
dpi = 80,
layer_range=None,
show_layer_activations=True,
)

for i, train_accuracy in enumerate(train_accuracy_history):
    plt.plot(train_accuracy, label=f'Fold {i+1}')

plt.ylim([0.85, 1.0])
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

for i, val_accuracy in enumerate(val_accuracy_history):
    plt.plot(val_accuracy, label=f'Fold {i+1}')

plt.ylim([0.85, 1.0])
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


for i, val_loss in enumerate(train_loss_history):
    plt.plot(val_loss, label=f'Fold {i+1}')

plt.ylim([0, 0.35])
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


for i, val_loss in enumerate(val_loss_history):
    plt.plot(val_loss, label=f'Fold {i+1}')

plt.ylim([0, 0.35])
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

fig, ax = plt.subplots()

for i, accuracy in enumerate(train_accuracy_history):
    ax.plot(accuracy, label=f'Train Accuracy Fold {i+1}')

for i, accuracy in enumerate(val_accuracy_history):
    ax.plot(accuracy, label=f'Validation Accuracy Fold {i+1}', linestyle=':')
      
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

fig, ax = plt.subplots()

for i, loss in enumerate(train_loss_history):
    ax.plot(loss, label=f'Train Loss Fold {i+1}')

for i, accuracy in enumerate(val_loss_history):
    ax.plot(accuracy, label=f'Validation Loss Fold {i+1}', linestyle=':')
    
plt.title('Training and Accuracy Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()